{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_to_seq",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvmaTB5ShKVx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ecf92b5-4a9c-41b1-ac6a-dc5279fe6cbc"
      },
      "source": [
        "# data 불러오기\n",
        "% cd /content\n",
        "data = []\n",
        "with open(\"data.txt\") as f:\n",
        "  for line in f.readlines():\n",
        "    data.append(line.strip())\n",
        "\n",
        "len(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11700"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbn8FYNE82j1"
      },
      "source": [
        "!pip install tensorflow==1.15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kNDF-Eh9Aa2",
        "outputId": "d015e8fa-125e-4110-c360-3dc8a9d0fc09"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "# 2.x 이 출력될 경우, 런타임 재시작 후 다시 확인"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0i7UVg_95aH",
        "outputId": "cebab640-96bb-4cae-8e38-7d57368789b5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l453EA0a9CwH"
      },
      "source": [
        "# 드라이브 내 test 파일 경로 설정\n",
        "% cd /content/drive/MyDrive/Colab Notebooks\n",
        "\n",
        "# 현재 디렉토리 내 파일 출력\n",
        "% ls "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK9nl4ZW9SyG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f3a2e9d-3c7f-49a9-8828-72b1e7fd69c9"
      },
      "source": [
        "from tokenizationK import FullTokenizer\n",
        "# ETRI 버트를 받으면 tokenization.py가 들어있는데 그거 말고 꼭 제가 제공해드린 tokenizationK.py를 쓰도록 해주세요.\n",
        "tokenizer = FullTokenizer(vocab_file=\"/content/drive/MyDrive/Colab Notebooks/vocab.korean.rawtext.list\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/drive/MyDrive/Colab Notebooks/tokenizationK.py:133: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZqL_636YL0W"
      },
      "source": [
        "import re\n",
        "\n",
        "class Seq:\n",
        "  def __init__(self, data):\n",
        "    self.data = self.prep(data)\n",
        "    self.seq_in = []\n",
        "    self.index = 0\n",
        "    self.is_slot = \"\"\n",
        "    self.seq_out = []\n",
        "    self.unmatched_word = \"\"\n",
        "    self.has_space = False\n",
        "\n",
        "  def prep(self, data):\n",
        "    # \"/\",\";\" 이외의 특수문자 제거\n",
        "    pat = re.compile(\"[^\\w\\s/;]\")\n",
        "    data = pat.sub(\"\",data)\n",
        "    # 슬롯 뒤에 띄어쓰기 추가\n",
        "    pat = re.compile(\"/\")\n",
        "    data = pat.sub(\"/ \",data)\n",
        "    # 이중공백을 공백으로 변환\n",
        "    pat = re.compile(\"\\s{2,}\")\n",
        "    data = pat.sub(\" \",data)\n",
        "    return data.lower()\n",
        "\n",
        "  def get_seq_in(self):\n",
        "    data = self.data\n",
        "\n",
        "    # 태그 제거\n",
        "    pat = re.compile(\"/\\s\\w+;\")\n",
        "    data = pat.sub(\"\",data)\n",
        "    # 특수문자 제거\n",
        "    pat = re.compile(\"[^\\w\\s]\")\n",
        "    data = pat.sub(\"\",data)\n",
        "    \n",
        "    data = tokenizer.tokenize(data)\n",
        "    self.seq_in = data\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "  def get_seq_out(self):\n",
        "    self.data = re.sub(\" \",\"_\",self.data)\n",
        "    for token in self.seq_in:\n",
        "      if self.index >= len(self.data):\n",
        "        break\n",
        "      self.seq_out.append(self.get_slot(token))\n",
        "      ####print(self.seq_out)\n",
        "    return self.seq_out\n",
        "\n",
        "  def slot_end(self):\n",
        "    subsentence = self.data[self.index:]\n",
        "    end_m = re.match(\"/\",subsentence)\n",
        "    if end_m:\n",
        "      self.index+=1\n",
        "      self.is_slot = \"\"\n",
        "      if self.has_space:\n",
        "        self.index+=1\n",
        "        self.has_space = False\n",
        "\n",
        "\n",
        "  def find_match(self,token):\n",
        "    matched_word = \"\"\n",
        "    pat = re.compile(token)\n",
        "    subsentence = self.data[self.index:]\n",
        "    m = pat.match(subsentence)\n",
        "    if not m:\n",
        "      if token[-1] == \"_\":\n",
        "        matched_word = token[:-1]\n",
        "        self.has_space = True\n",
        "      else:\n",
        "        if self.unmatched_word:\n",
        "          matched_word = combine(self.unmatched_word,token)\n",
        "          self.unmatched_word = \"\"\n",
        "        else:\n",
        "          self.unmatched_word = token\n",
        "    else:\n",
        "      matched_word = m.group()\n",
        "    return matched_word\n",
        "\n",
        "\n",
        "  def slot_start(self):\n",
        "    subsentence = self.data[self.index:]\n",
        "    if subsentence[0]=='/':\n",
        "      temp = re.match(\"/\\w+;\",subsentence).group()\n",
        "      self.index+=len(temp)\n",
        "      self.is_slot = temp[1:-1]\n",
        "\n",
        "  def get_slot(self, token):\n",
        "    pat = re.compile(token)\n",
        "    if self.is_slot:\n",
        "      self.slot_end()\n",
        "    if not self.is_slot:\n",
        "      self.slot_start()\n",
        "    matched_word = self.find_match(token)\n",
        "    self.index+=len(matched_word)\n",
        "\n",
        "    if self.is_slot:\n",
        "      return self.is_slot[1:]\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkolaJdOqZID"
      },
      "source": [
        "# 종성 리스트. 00 ~ 27 + 1(1개 없음)\n",
        "JONGSUNG_LIST = [' ', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
        "\n",
        "def combine(word, jongsung):\n",
        "  if jongsung == 'ᆫ':\n",
        "    jongsung = 'ㄴ'\n",
        "  if jongsung == 'ᆼ':\n",
        "    jongsung = 'ㅇ'\n",
        "  if jongsung == '[UNK]':\n",
        "    jongsung = 'ㅂ'\n",
        "  if jongsung == 'ᆷ':\n",
        "    jongsung = 'ㅁ'\n",
        "  if jongsung == 'ᆯ':\n",
        "    jongsung = 'ㄹ'\n",
        "  return word[:-1]+chr(ord(word[-1])+JONGSUNG_LIST.index(jongsung))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zq7vVZ9QdVC"
      },
      "source": [
        "seq_in = []\n",
        "seq_out = []\n",
        "\n",
        "for sentence in data:\n",
        "  s = Seq(sentence)\n",
        "  seq_in.append(s.get_seq_in())\n",
        " # try:\n",
        "  seq_out.append(s.get_seq_out())\n",
        "  #except:\n",
        "  #  print(s.seq_in)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHWKw1_fTH9m"
      },
      "source": [
        "import random\n",
        "\n",
        "index = list(range(len(seq_in)))\n",
        "\n",
        "i = random.choice(index)\n",
        "\n",
        "for j in range(len(seq_in[i])):\n",
        "  print(seq_in[i][j], seq_out[i][j])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcestaaLVDlC"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "seq = pd.DataFrame({\"seq_in\":seq_in, \"seq_out\":seq_out})\n",
        "seq.to_pickle(\"seq.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx0f6e7vaRxt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}